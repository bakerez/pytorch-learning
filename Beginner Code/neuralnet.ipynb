{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model Without Training\n",
    "Import in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that pytorch is running on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural net model class. We will have an input layer, one hidden layer, and an output layer. nn.Relu simply changes all negative numbers into zeros. nn.Linear performs linear transformations of the inputs to obtain the outputs for each of the nodes. nn.Flatten will flatten input tensor, into less dimensions (ex [[1,2],[3,4]] becomes [1,2,3,4])\n",
    "\n",
    "> **Questions:**\n",
    ">- Why do we use the ReLU function in this case instead of other activation functions?\n",
    ">- Why do we use 512 as the hidden layer size? Is this arbitrary, or is there some way to make a good decision for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # ...idk about this line...\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # I think flattens an array/tensor??\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # arguments of nn.Linear are (# of input nodes, # of output nodes) for the given layer\n",
    "            nn.Linear(28*28, 512),\n",
    "            # Q: Why do we use ReLU here instead of sigmoid or other function??\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # Passes input data throught the model and returns the output\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the model, put onto gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random test input and pass it through the (untrained) model. Let's check to see what weights/biases were automatically assigned to the model at creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3], device='cuda:0')\n",
      "First Linear weights: Parameter containing:\n",
      "tensor([[ 0.0293, -0.0008,  0.0235,  ..., -0.0003, -0.0275,  0.0145],\n",
      "        [ 0.0185, -0.0049,  0.0257,  ...,  0.0131,  0.0317, -0.0196],\n",
      "        [-0.0239, -0.0062, -0.0054,  ...,  0.0312,  0.0029, -0.0082],\n",
      "        ...,\n",
      "        [-0.0259,  0.0107, -0.0184,  ...,  0.0054,  0.0082, -0.0226],\n",
      "        [-0.0276,  0.0050, -0.0124,  ...,  0.0068, -0.0216, -0.0284],\n",
      "        [-0.0026, -0.0097,  0.0314,  ...,  0.0069, -0.0100,  0.0121]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n",
      "First Linear weights: Parameter containing:\n",
      "tensor([-1.7845e-02,  2.9038e-02, -2.8424e-02,  3.0901e-02,  1.2755e-02,\n",
      "        -1.8704e-02,  2.2038e-03,  1.4839e-02,  3.1548e-02, -9.1306e-03,\n",
      "        -7.4122e-03, -3.0813e-03, -1.7229e-02, -2.4003e-02, -3.5428e-02,\n",
      "         3.3135e-02, -2.2036e-02, -3.8221e-03, -3.5489e-02,  2.0112e-02,\n",
      "         1.9395e-02, -1.5363e-02,  6.0703e-03,  7.5113e-03, -2.9885e-02,\n",
      "         3.2538e-04,  7.1442e-04,  7.3937e-03,  1.5798e-02,  1.0386e-02,\n",
      "        -3.6341e-03, -1.4820e-02,  3.1610e-02,  1.9995e-02, -1.5602e-02,\n",
      "         2.3058e-02,  1.5043e-03,  2.2921e-02, -2.4934e-02,  7.9151e-03,\n",
      "         1.9380e-02, -2.1663e-02,  3.0479e-02,  5.9045e-03,  2.4215e-02,\n",
      "         9.0540e-03, -1.6215e-03, -3.3646e-02,  2.5742e-02,  2.9824e-02,\n",
      "        -1.0976e-02,  2.0467e-02, -2.5691e-02, -2.9136e-02,  3.0252e-02,\n",
      "        -9.2877e-03,  2.6627e-02,  3.1744e-02, -3.0030e-02, -1.0142e-03,\n",
      "        -2.3814e-02,  2.4820e-02, -1.5778e-02,  2.4554e-04, -2.0217e-02,\n",
      "         2.8802e-02,  3.0618e-02,  2.3675e-02, -2.1963e-04, -3.0314e-02,\n",
      "         3.0670e-02, -8.4542e-03, -1.4662e-03, -1.0470e-02,  2.0746e-02,\n",
      "        -7.4391e-03,  1.2540e-02, -2.0886e-02,  3.6867e-03,  2.0078e-02,\n",
      "         1.5660e-02,  3.5653e-02, -1.2902e-02, -3.0711e-02, -5.4728e-03,\n",
      "         3.4789e-02, -1.4406e-02,  3.8129e-03,  2.3626e-03, -5.1886e-03,\n",
      "        -1.3734e-03, -1.0540e-02,  3.5499e-03,  3.4410e-02,  2.1399e-02,\n",
      "        -1.0235e-02,  2.7908e-02, -2.4873e-02,  2.5904e-02, -5.7336e-03,\n",
      "         1.7847e-02, -2.0610e-02, -3.3654e-02,  7.6578e-03,  8.2343e-03,\n",
      "         1.0116e-02,  6.9084e-03,  2.1452e-02,  3.4833e-02,  3.0601e-02,\n",
      "         2.1862e-02,  2.5577e-02, -2.2105e-02, -5.6833e-04,  1.7724e-02,\n",
      "        -6.9955e-03, -7.4916e-03, -3.0247e-02,  2.3786e-02, -5.6266e-03,\n",
      "        -3.4464e-02, -6.7719e-03,  1.3897e-02, -3.2292e-02, -4.7577e-03,\n",
      "        -7.9138e-03,  2.3572e-02, -3.2856e-02, -2.6571e-02, -3.4290e-02,\n",
      "         1.6487e-02,  2.3363e-02,  9.4177e-03, -2.9348e-02,  1.8633e-02,\n",
      "        -3.3558e-02, -4.6641e-03,  3.4733e-02, -1.0379e-03, -9.3620e-03,\n",
      "        -6.0302e-03,  3.0262e-03, -1.8589e-02,  1.0265e-02,  2.9159e-02,\n",
      "        -3.4068e-02,  3.0375e-02,  5.9107e-03, -7.3891e-03, -2.9875e-02,\n",
      "         1.3086e-02,  2.8199e-02, -2.5543e-02, -2.4735e-02, -4.8179e-03,\n",
      "         2.3084e-02, -3.2638e-02, -2.5407e-02,  3.1827e-03,  2.7175e-02,\n",
      "        -3.3668e-02,  1.0864e-02,  3.4511e-02, -2.3620e-02,  2.0020e-02,\n",
      "        -4.1413e-04,  2.0893e-02, -6.0856e-03, -1.7459e-03,  1.4102e-02,\n",
      "         2.9992e-03, -3.4650e-02, -2.4693e-02,  1.6551e-02, -2.3305e-02,\n",
      "        -2.8725e-03, -2.8704e-03,  3.2062e-02, -2.0169e-03, -1.7279e-02,\n",
      "        -2.4345e-04, -5.5100e-03,  1.3642e-03, -2.0213e-02, -1.6507e-02,\n",
      "         3.2287e-02,  3.4195e-02, -2.8999e-02, -3.4529e-02, -1.0862e-02,\n",
      "         1.2325e-02, -1.4864e-02, -4.0249e-03,  2.0224e-03, -3.1671e-02,\n",
      "         5.3216e-03,  2.4174e-02,  3.4645e-02,  9.3604e-03, -1.8710e-02,\n",
      "        -4.1410e-03,  2.2821e-02, -7.2600e-04,  1.4984e-02,  2.9238e-02,\n",
      "        -1.7989e-02, -4.8119e-03, -3.3345e-02, -2.3710e-02, -1.2490e-02,\n",
      "         2.9691e-02,  1.7558e-02,  1.2064e-02, -2.2750e-02,  2.1194e-05,\n",
      "        -8.9767e-03, -3.0358e-02, -2.8527e-02,  1.1520e-02, -2.5643e-03,\n",
      "        -1.9463e-02,  9.2555e-03, -2.0546e-02,  1.5643e-02, -1.2999e-03,\n",
      "        -8.7957e-03, -1.1180e-02, -5.0487e-03, -1.4637e-02,  1.1590e-02,\n",
      "        -1.7660e-02, -4.4395e-03,  4.6553e-03,  2.4561e-02, -1.7851e-03,\n",
      "        -1.0387e-02,  1.1560e-02,  9.0633e-03,  1.2524e-02, -2.4396e-02,\n",
      "        -1.6988e-02, -4.4863e-03,  2.3645e-02,  1.9007e-02,  1.2663e-02,\n",
      "         4.2419e-03,  1.6170e-02, -1.2396e-02, -2.1928e-02, -1.3401e-02,\n",
      "        -1.2674e-02,  2.1766e-02,  3.4143e-03,  1.4937e-02, -7.1822e-03,\n",
      "         2.0069e-02,  7.8311e-03, -2.1623e-02,  1.2917e-02,  2.5936e-02,\n",
      "        -1.7397e-02,  3.4800e-02, -2.5799e-02,  1.6922e-02,  1.5122e-02,\n",
      "        -2.5868e-02, -3.5188e-02,  2.7740e-02, -3.3510e-02, -2.0068e-02,\n",
      "        -9.0205e-03, -3.3165e-02,  9.8478e-03,  3.2777e-02, -3.3545e-03,\n",
      "        -1.6659e-03, -2.0274e-02, -3.1079e-02,  1.1511e-02, -3.1517e-02,\n",
      "        -2.9729e-02,  1.8649e-02, -1.8982e-02, -2.2167e-02,  2.7743e-02,\n",
      "         7.4248e-03,  9.9932e-04, -2.6845e-02,  1.7682e-02,  7.1440e-03,\n",
      "        -9.7454e-03,  3.5098e-03, -3.2706e-02,  3.6123e-03,  1.5094e-03,\n",
      "         8.6978e-03,  3.3619e-02,  2.2036e-02,  6.2338e-04, -2.7244e-02,\n",
      "        -2.4803e-02,  7.6791e-03, -2.4421e-02, -2.8761e-02,  3.3988e-02,\n",
      "         1.1907e-02, -1.7895e-02, -2.8582e-02, -3.2279e-02, -2.7731e-02,\n",
      "         1.6525e-02, -3.6860e-03,  2.6856e-02,  1.3857e-02, -2.1194e-02,\n",
      "         1.8948e-02, -2.5090e-03, -6.4074e-04,  1.2662e-02,  2.7944e-02,\n",
      "         5.3483e-03,  2.7594e-02,  2.0860e-02,  2.7424e-03,  5.0964e-03,\n",
      "        -2.7145e-02, -7.3799e-05, -1.7788e-02,  1.6908e-02, -7.1479e-04,\n",
      "        -6.6211e-03,  1.5010e-02,  9.9011e-04, -1.6116e-02, -2.5410e-02,\n",
      "        -2.2995e-02, -2.5062e-02,  4.0390e-03, -2.6728e-03,  3.3788e-02,\n",
      "        -3.3654e-02,  1.2011e-02, -5.7088e-03,  9.3566e-03, -2.5080e-02,\n",
      "        -3.2083e-02, -1.2221e-02, -1.3516e-02, -2.7128e-02,  3.0556e-02,\n",
      "         3.3663e-03, -2.5516e-04,  1.2299e-02,  2.4160e-02, -3.5095e-02,\n",
      "         1.5090e-02,  3.1074e-02, -5.3912e-04,  3.5176e-02, -3.3972e-02,\n",
      "        -3.3888e-02,  1.7984e-02,  9.9603e-03, -1.5832e-02,  2.5369e-02,\n",
      "         6.7976e-03,  8.5439e-03,  1.3703e-02, -5.1178e-03,  1.6506e-02,\n",
      "        -2.0465e-02, -1.9274e-02, -3.2117e-02,  1.5861e-02, -1.3578e-02,\n",
      "         3.2612e-02,  9.4366e-04,  1.4612e-02,  3.3029e-02,  2.9343e-02,\n",
      "         3.3403e-02,  1.0583e-02, -2.3973e-02, -2.0024e-02, -6.4197e-03,\n",
      "         2.9696e-02,  2.8674e-02,  2.3342e-02, -6.9862e-03,  1.6283e-02,\n",
      "         5.0110e-03, -1.7393e-03,  4.9217e-03, -1.7817e-02,  9.2095e-03,\n",
      "        -3.2409e-02, -2.7558e-02, -2.0926e-02,  2.2871e-02, -2.2419e-02,\n",
      "         1.8765e-02, -6.4535e-03, -1.8059e-02,  1.1273e-03, -3.5607e-02,\n",
      "         8.8235e-03,  2.4904e-02,  2.3860e-02, -5.4492e-03, -2.1059e-02,\n",
      "        -2.6438e-02,  2.7155e-02,  2.9487e-02, -2.5602e-02,  2.3859e-02,\n",
      "         2.9332e-02,  9.1389e-03, -2.5660e-02,  3.6098e-03, -3.2654e-02,\n",
      "         3.0890e-02,  2.0959e-02, -1.8054e-02,  3.6166e-03,  2.1911e-03,\n",
      "        -1.6786e-02,  2.7037e-02, -3.4027e-02, -2.1441e-02,  2.9773e-02,\n",
      "        -2.3715e-02,  1.2010e-02, -1.7267e-02,  3.0363e-02, -1.6371e-02,\n",
      "         2.7804e-02, -1.2420e-02,  5.8530e-03, -5.4658e-03, -2.0648e-03,\n",
      "         2.4005e-02,  2.9293e-02,  9.3867e-03,  2.5488e-02, -2.6445e-02,\n",
      "        -4.7514e-03, -2.9785e-02,  2.9357e-02,  2.0876e-02,  1.3255e-02,\n",
      "         5.8899e-03,  2.2096e-02, -1.5618e-02,  5.4166e-03,  1.4355e-02,\n",
      "         2.9127e-02, -1.5682e-02,  4.0581e-03, -3.2191e-02,  1.8461e-02,\n",
      "         2.8444e-02,  3.2527e-03, -2.5071e-02,  2.3194e-02, -3.1755e-02,\n",
      "        -2.3488e-03,  3.8166e-03, -2.1566e-02,  2.0705e-02,  1.1452e-02,\n",
      "         1.0436e-02, -3.7292e-03,  2.1130e-02,  3.2955e-02, -1.4823e-02,\n",
      "         2.9294e-02, -3.3880e-02, -2.9447e-02,  3.3183e-02,  2.0079e-02,\n",
      "         1.9099e-02, -3.5618e-02, -2.4407e-02,  3.2902e-02,  1.4701e-02,\n",
      "         3.2373e-02,  2.4630e-02, -1.7769e-02,  6.7313e-03, -1.9639e-04,\n",
      "         1.3876e-02, -8.7999e-03,  3.3385e-02,  1.0882e-02, -3.4264e-02,\n",
      "        -8.0753e-03, -2.2553e-02, -2.5893e-02,  1.3935e-02, -5.6125e-03,\n",
      "        -2.1587e-02,  3.5429e-02,  4.7734e-03,  1.5960e-02, -6.7280e-03,\n",
      "        -3.2895e-02,  8.8533e-03,  2.2669e-02,  2.7944e-03, -1.8150e-02,\n",
      "        -1.4043e-02, -6.0207e-03], device='cuda:0', requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "\n",
    "# Softmax: converts numbers from the output layer from infinite range into a probability in [0,1]\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "\n",
    "# Which output node has the highest probability? That is the predicted class!\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "\n",
    "print(f\"First Linear weights: {model.linear_relu_stack[0].weight} \\n\")\n",
    "\n",
    "print(f\"First Linear weights: {model.linear_relu_stack[0].bias} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
